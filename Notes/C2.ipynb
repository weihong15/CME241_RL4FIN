{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MDP\n",
    "* Stationary: $\\mathbb{P}[(R_{t+1},S_{t+1})|(S_t,A_t)]$is independent of t\n",
    "* State-Reward Transition Probability Function $\\mathcal{P}_R:\\mathcal{N} \\times \\mathcal{A} \\times \\mathcal{D} \\times \\mathcal{S}\\rightarrow[0,1]$, conceptualize in python as $\\mathcal{N}\\rightarrow (\\mathcal{A}\\rightarrow (\\mathcal{S} \\times \\mathcal{D}\\rightarrow[0,1]))$\n",
    "\\begin{equation}\n",
    "\\mathcal{P}_R(s,a,r,s') = \\mathbb{P}[(R_{t+1}=r,S_{t+1}=s')|(S_t=s,A_t=a)]\n",
    "\\end{equation}\n",
    "```\n",
    "StateReward = FiniteDistribution[Tuple[S, float]]\n",
    "ActionMapping = Mapping[A, StateReward[S]]\n",
    "StateActionMapping = Mapping[S, Optional[ActionMapping[A, S]]]\n",
    "```\n",
    "* State Transition Probability Function $\\mathcal{N}\\times\\mathcal{A}\\times S\\rightarrow [0,1]$\n",
    "\\begin{equation}\n",
    "\\mathcal{P}(s,a,s') = \\sum_{R\\in \\mathcal{D}} \\mathcal{P}_R(s,a,r,s')\n",
    "\\end{equation}\n",
    "\n",
    "* Reward Transition Function $\\mathcal{R}_T:\\mathcal{N}\\times\\mathcal{A}\\times S\\rightarrow\\mathbb{R} $\n",
    "\\begin{equation}\n",
    "\\mathcal{R}_T(s,a,s') = \\mathbb{E}[R_{t+1}|(S_{t+1}=s',S_t=s,A_t=a)] \\\\\n",
    "= \\sum_{R\\in \\mathcal{D}} \\frac{\\mathcal{P}_R(s,a,r,s')}{\\mathcal{P}(s,a,s')} \\cdot r = \\sum_{R\\in \\mathcal{D}} \\frac{\\mathcal{P}_R(s,a,r,s')}{\\sum_{R\\in \\mathcal{D}} \\mathcal{P}_R(s,a,r,s')} \\cdot r\n",
    "\\end{equation}\n",
    "\n",
    "* Reward Function $\\mathcal{R}:\\mathcal{N}\\times\\mathcal{A}\\rightarrow\\mathbb{R} $\n",
    "\\begin{equation}\n",
    "\\mathcal{R}_T(s,a) = \\mathbb{E}[R_{t+1}|(S_t=s,A_t=a)] \\\\\n",
    "= \\sum_{s'\\in\\mathcal{S}} \\mathcal{P}(s,a,s')\\cdot \\mathcal{R}_T(s,a,s') =\\sum_{s'\\in\\mathcal{S}}\\sum_{r\\in \\mathbb{R}}\\mathcal{P}_R(s,a,r,s') \\cdot r\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy\n",
    "A policy is an Agent-controlled function $\\pi:\\mathcal{N}\\times\\mathcal{A}\\rightarrow[0,1]$, probability of choosing an action given a state, probability distribution of actions\n",
    "\\begin{equation}\n",
    "\\pi (s,a) =\\mathbb{P}[A_t=a|S_t=s] \\text{ for all time steps $t=0,1,2,\\dots$}\n",
    "\\end{equation}\n",
    "\n",
    "## Deterministic policy\n",
    "We definitely choose an action given a state, single action instead of distribution. $\\pi_D:\\mathcal{N}\\rightarrow\\mathcal{A}$\n",
    "\\begin{equation}\n",
    "\\pi(s,\\pi_D(s))=1 \\text{ and } \\pi(s,a)=0 \\text{ for all $a\\in\\mathcal{A}$ with a $\\neq \\pi_D(s)$ }\n",
    "\\end{equation}\n",
    "\n",
    "## probability and rewards implied by policy\n",
    "\\begin{equation}\n",
    "\\mathcal{P}_R^\\pi(s,r,s')=\\sum_{a\\in\\mathcal{A}} \\pi(s,a)\\cdot\\mathcal{P}_R(s,a,r,s')\\\\\n",
    "\\mathcal{P}^\\pi(s,s')=\\sum_{a\\in\\mathcal{A}} \\pi(s,a)\\cdot\\mathcal{P}(s,a,s')\\\\\n",
    "\\mathcal{R}_T^\\pi(s,s')=\\sum_{a\\in\\mathcal{A}} \\pi(s,a)\\cdot\\mathcal{R}_T(s,a,s')\\\\\n",
    "\\mathcal{R}^\\pi(s)=\\sum_{a\\in\\mathcal{A}} \\pi(s,a)\\cdot\\mathcal{R}(s,a)\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# State-Value Function(implied by policy $\\pi$\n",
    "* State-Value Function(for policy $\\pi$) $V^\\pi:\\mathcal{N}\\rightarrow\\mathbb{R}$ is defined as:\n",
    "\\begin{equation}\n",
    "V^\\pi(s)=\\mathbb{E}_{\\pi,\\mathcal{P}_R}[G_t|S_t=s]\\text{ for all $s\\in\\mathcal{N}$, for all $t=0,1,2,\\dots$}\n",
    "\\end{equation}\n",
    "* MRP Bellman equation\n",
    "\\begin{equation}\n",
    "V^\\pi(s)= \\mathcal{R}^\\pi(s)+\\gamma\\cdot\\sum_{s'\\in\\mathcal{N}}\\mathcal{P}^\\pi(s,s')\\cdot V^\\pi(s')\n",
    "\\end{equation}\n",
    "* MRP bellman equation in terms of MDP\n",
    "\\begin{equation}\n",
    "V^\\pi(s)= \\sum_{a\\in\\mathcal{A}}\\pi(s,a)\\cdot\\left(\\mathcal{R}(s,a)+\\gamma\\cdot\\sum_{s'\\in\\mathcal{N}}\\mathcal{P}(s,a,s')\\cdot V^\\pi(s')\\right)\\text{ for $s\\in\\mathcal{N}$}\n",
    "\\end{equation}\n",
    "\n",
    "# Action-Value Function (Q function) of an MDP for a fixed policy\n",
    "* Action-Value Function(for policy $\\pi$) $Q^\\pi :\\mathcal{N}\\times\\mathcal{A}\\rightarrow\\mathbb{R}$ defined as:\n",
    "\\begin{equation}\n",
    "Q^\\pi(s,a)=\\mathbb{E}_{\\pi,\\mathcal{P}_R}[G_t|(S_t=s,A_t=a)] \\text{ for all $s\\in\\mathcal{N}, a\\in \\mathcal{A}$}\\\\\n",
    "V^\\pi(s)=\\sum_{a\\in\\mathcal{A}} \\pi(s,a)\\cdot Q^\\pi(s,a) \\text{ $s\\in\\mathcal{N}$}\n",
    "\\end{equation}\n",
    "Combining the last few questions, we get different formulation of Q\n",
    "\\begin{equation}\n",
    "Q^\\pi(s,a)=\\mathcal{R}(s,a)+\\gamma\\cdot\\sum_{s'\\in\\mathcal{N}}\\mathcal{P}(s,a,s')\\cdot V^\\pi(s')\\text{ for all $s\\in\\mathcal{N}, a\\in \\mathcal{A}$}\\\\\n",
    "Q^\\pi(s,a)=\\mathcal{R}(s,a)+\\gamma\\cdot\\sum_{s'\\in\\mathcal{N}}\\mathcal{P}(s,a,s')\\sum_{a'\\in\\mathcal{A}} \\text{ for all $s\\in\\mathcal{N}, a\\in \\mathcal{A}$}\\\\\n",
    "\\pi(s',a')\\cdot Q^\\pi(s',a')\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimal functions\n",
    "1. State Value function, v\n",
    "2. action value function, q\n",
    "* Optimal State value function $V^* :\\mathcal{N} \\rightarrow\\mathbb{R}$ defined as:\n",
    "\\begin{equation}\n",
    "V^*(s) = \\max_{\\pi\\in\\Pi} V^\\pi(s)\\text{ for all $s\\in\\mathcal{N}$}\n",
    "\\end{equation}\n",
    "* Optimal Action-Value function $Q^*:\\mathcal{N}\\times\\mathcal{A}\\rightarrow\\mathbb{R}$ defined as:\n",
    "\\begin{equation}\n",
    "Q^*(s,a)=\\max_{\\pi\\in\\Pi} Q^\\pi(s,a) \\text{ for all $s\\in\\mathcal{N},a\\in\\mathcal{A}$}\n",
    "\\end{equation}\n",
    "\n",
    "Note that \n",
    "\\begin{equation}\n",
    "V^*(s) = \\max_{a\\in\\mathcal{A}} Q^*(s,a) \\\\\n",
    "Q^*(s,a)=\\mathcal{R}(s,a)+\\gamma\\cdot\\sum_{s'\\in\\mathcal{N}}\\mathcal{P}(s,a,s')\\cdot V^*(s')\n",
    "\\end{equation}\n",
    "\n",
    "# MDP State-Value Function Bellman Optimality Equation\n",
    "\\begin{equation}\n",
    "V^*(s) = \\max_{a\\in\\mathcal{A}}\\{\\mathcal{R}(s,a)+\\gamma\\cdot\\sum_{s'\\in\\mathcal{N}}\\mathcal{P}(s,a,s')\\cdot V^*(s')\\}\n",
    "\\end{equation}\n",
    "\n",
    "# MDP Action-Value Function Bellman\n",
    "\\begin{equation}\n",
    "Q^*(s,a)=\\mathcal{R}(s,a)+\\gamma\\cdot\\sum_{s'\\in\\mathcal{N}}\\mathcal{P}(s,a,s')\\cdot \\max_{a\\in\\mathcal{A}} Q^*(s,a)\n",
    "\\end{equation}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FiniteMarkovProcess\n",
    "```\n",
    "100 is a Terminal State\n",
    "From State 0:\n",
    "  To State 38 with Probability 0.167\n",
    "  To State 2 with Probability 0.167\n",
    "  To State 3 with Probability 0.167\n",
    "  To State 14 with Probability 0.167\n",
    "  To State 5 with Probability 0.167\n",
    "  To State 6 with Probability 0.167\n",
    "From State 2:\n",
    "```\n",
    "\n",
    "## FiniteMarkovRewardProcess\n",
    "```\n",
    "From State 0:\n",
    "  To [State 38 and Reward 1.000] with Probability 0.167\n",
    "  To [State 2 and Reward 1.000] with Probability 0.167\n",
    "  To [State 3 and Reward 1.000] with Probability 0.167\n",
    "  To [State 14 and Reward 1.000] with Probability 0.167\n",
    "  To [State 5 and Reward 1.000] with Probability 0.167\n",
    "  To [State 6 and Reward 1.000] with Probability 0.167\n",
    "From State 2:\n",
    "  To [State 3 and Reward 1.000] with Probability 0.167\n",
    "  To [State 14 and Reward 1.000] with Probability 0.167\n",
    "```\n",
    "\n",
    "## FiniteMarkovDecisionProcess\n",
    "\n",
    "```\n",
    "From State 1:\n",
    "  With Action A:\n",
    "    To [State 0 and Reward -1.000] with Probability 0.333\n",
    "    To [State 2 and Reward 0.000] with Probability 0.667\n",
    "  With Action B:\n",
    "    To [State 0 and Reward -1.000] with Probability 0.333\n",
    "    To [State 2 and Reward 0.000] with Probability 0.333\n",
    "    To [State 3 and Reward 1.000] with Probability 0.333\n",
    "From State 2:\n",
    "  With Action A:\n",
    "    To [State 1 and Reward 0.000] with Probability 0.667\n",
    "    To [State 3 and Reward 1.000] with Probability 0.333\n",
    "  With Action B:\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
