{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, dynamic programming is not about recursive algorithms with overlapping subproblems. We restrict DP to \"Algorithms for Prediction and Control\"\n",
    "1. Prediction: Compute the Value Function of an MRP\n",
    "2. Control: Compute the Optimal Value Function of an MDP //Optimal policy can be extracted from OVF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fixed point and Banach Fixed-Point Theorem\n",
    "\n",
    "Defintion.  \n",
    "Fix point of a function f, is a value x such that $x=f(x)$.  \n",
    "\n",
    "## Banach Fixed-point THeorem\n",
    "Let $\\mathcal{X}$ be a non-empty set equipped with a complete metric $d:\\mathcal{X}\\times \\mathcal{X} \\rightarrow\\mathbb{R}$.  Let$f:\\mathcal{X}\\rightarrow\\mathcal{X}$ be such that there exists a $L \\in[0,1)$ such that $d(f(x_1),f(x_2))≤L\\cdot d(x_1,x_2)$ for all $x_1,x_2 \\in X$.  Then\n",
    "1. There exist a unique Fixed-point $x^* = \\mathcal{X}$ \n",
    "\\begin{equation}\n",
    "x^*=f(x^*)\n",
    "\\end{equation}\n",
    "2. For any $x_0\\in\\mathcal{X}$, and sequence $[x_i|i=0,1,2,...]$ defined as $x_{i+1} = f(x_i)$ for all $i=0,1,2,\\dots$\n",
    "\\begin{equation}\n",
    "\\lim_{i\\rightarrow \\infty} x_i = x^*\n",
    "\\end{equation}\n",
    "\n",
    "In summary, if u have a distance metric that contracts a y axis value, it will have a fixed point in there. We can iteratively apply f to converge the function\n",
    "\n",
    "### Example\n",
    "we can show $y = cos(x)$ is a contraction, to find the fix point(i.e $x=cos(x)$, we can just pick any number $x_0$ and repeatedly apply $cos(...(cos(x_0)...)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Value Function Iteration\n",
    "\n",
    "In essnsce, we are trying to solve the **MRP Bellman Equation**:\n",
    "\\begin{equation}\n",
    "\\mathbf{V}^\\pi = \\mathbf{\\mathcal{R}}^\\pi+\\gamma\\mathbf{\\mathcal{P}}^\\pi \\cdot \\mathbf{V}^\\pi\n",
    "\\end{equation}\n",
    "\n",
    "For non-large spaces, we can just invert the matrix and compute V\n",
    "\\begin{equation}\n",
    "\\mathbf{V}^\\pi = (\\mathbf{I}_m - \\gamma\\mathbf{\\mathcal{P}}^\\pi)^{-1}\\cdot\\mathbf{\\mathcal{R}}^\\pi\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the **MRP Bellman Equation**, we define a function $B^\\pi :\\mathbb{R}^m\\rightarrow \\mathbb{R}^m$ as:\n",
    "\\begin{equation}\n",
    "B^\\pi(\\mathbf{V}) = \\mathbf{\\mathcal{R}}^\\pi+\\gamma\\mathbf{\\mathcal{P}}^\\pi \\cdot \\mathbf{V}^\\pi \\text{ for any Value Function vector $\\mathbf{V} \\in \\mathbb{R}^m$}\n",
    "\\end{equation}\n",
    "\n",
    "Our aim now is to solve for the fix point of function $B^\\pi$. We can further prove by using $L^\\infty$ norm as metric, we can prove for contraction and can apply the **banach fixed-point theorem**.\n",
    "\n",
    "## Policy Evaluation Convergence Theorem\n",
    "For a Finite MDP with $|\\mathcal{N}|=m$ and $\\gamma <1$, if $\\mathbf{V}^\\pi\\in\\mathbb{R}^m$ is the ValueFunction of the MDP when evaluated with a fixed policy $\\pi:\\mathcal{N} \\times \\mathcal{A}\\rightarrow [0,1]$, then $\\mathbf{V}^\\pi$ is the unique Fixed-Point of the BellmanPolicy Operator $B^\\pi:\\mathbb{R}^m\\rightarrow\\mathbb{R}^m$, and\n",
    "\\begin{equation}\n",
    "\\lim_{i\\rightarrow\\infty}(B^\\pi)^i(\\mathbf{V}_0)\\rightarrow\\mathbf{V}^\\pi \\text{ for all starting Value Functions $\\mathbf{V}_0\\in\\mathbb{R}^m$}\n",
    "\\end{equation}\n",
    "\n",
    "Note that $(B^\\pi)^i$ is applying function B repeatedly for i times. So from the above theorem, we can start with any value state and apply $B^\\pi$ repeatedly.\n",
    "\n",
    "## Policy Evaluation algorithm\n",
    "1. Start with any Value Function $\\mathbf{V}_0 \\in \\mathbb{R}^m$\n",
    "2. Iterating over $i=0,1,2,\\dots$ calculate in each iteration:\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbf{V}_{i+1}= B^\\pi(\\mathbf{V}_i) = \\mathbf{\\mathcal{R}}^\\pi+\\gamma\\mathbf{\\mathcal{P}}^\\pi \\cdot \\mathbf{V}_i\n",
    "\\end{equation}\n",
    "3. Stop iteration when $\\mathbf{V}_{i+1}$ is close to $\\mathbf{V}_{i}$\n",
    "\n",
    "Banach Fixed-Point THeorem also assures speed of convergence, depending on starting value function $V_0$ and on choice of $\\gamma$.  \n",
    "Running time of each iteration is $O(m^2)$ Constructing the MRP frmo the MDP and the policy takes $O(m^2k)$ where $m=|\\mathcal{N}|, k = |\\mathcal{A}|$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Greedy policy\n",
    "Now we will try to find another iterative algorithm that will improve our policy over time. We define **Greedy Policy Function** $G:\\mathbb{R}^m \\rightarrow (\\mathcal{N} \\rightarrow \\mathcal{A})$(interpreted as a function mapping a Value Function vector $\\mathbf{V}$ to a deterministic policy $\\pi'_D:\\mathcal{N}\\rightarrow\\mathcal{A}$\n",
    "\n",
    "\\begin{equation}\n",
    "G(\\mathbf{V})(s) = \\pi'_D(s) = \\arg \\max_{a\\in \\mathcal{A}} \\{ \\mathcal{R}(s,a) +\\gamma \\sum_{s'\\in\\mathcal{N}}\\mathcal{P}(s,a,s')\\cdot \\mathbf{V}(s')\\}\n",
    "\\end{equation}\n",
    "\n",
    "Note that what we are doing argmax on is the Q value function. we are finding the best action for q value function\n",
    "## Value Function Comparison Definition\n",
    "To compare between 2 policies, we will compare the value function assortiated with that policy.  \n",
    "We say $X\\geq Y$ for Value Functions $X,Y:\\mathcal{N}\\rightarrow \\mathbb{R}$ of an MDP iff:\n",
    "\\begin{equation}\n",
    "X(s)\\geq Y(s) \\text{ for all $s\\in\\mathcal{N}$}\n",
    "\\end{equation}\n",
    "We say that $\\pi_1$ is an improvement over $\\pi_2$ if $\\mathbf{V}^{\\pi_1}\\geq \\mathbf{V}^{\\pi_2}$\n",
    "\n",
    "## Policy Improvement Theorem\n",
    "FOr a finite MDP, for any policy $\\pi$\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbf{V}^{\\pi'_D}=\\mathbf{V}^{G(V^\\pi)}\\geq\\mathbf{V}^\\pi\n",
    "\\end{equation}\n",
    "\n",
    "Linking this policy back to Value Function Iteration, Note that applying $\\mathbf{B}^{\\pi'_D}=\\mathbf{B}^{G(V^\\pi)}$ repeatedly, starting with $\\mathbf{V}^\\pi$, will converge to $\\mathbf{B}^{\\pi'_D}$ (policy evaluation with policy $\\pi'_D = G(\\mathbf{V}^\\pi))$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes:\n",
    "G is a function that is applied to V and gives us a better policy policy(s)  \n",
    "B is also a function that is applied to V and give us a more accurate value function(true value will converge)  \n",
    "\n",
    "### Intuitive understanding of policy improvement theorem\n",
    "1. stage 0: Value Function $\\mathbf{V}^\\pi$ means execute policy $\\pi$ throughout\n",
    "2. Stage 1: VF $\\mathbf{B}^{\\pi'_D}(\\mathbf{V}^\\pi)$ means execute improved policy $\\pi'_D$ for the 1st time step and execute policy $\\pi$ for the rest of time steps\n",
    "3. This will improved the VF from Stage 0: $\\mathbf{V}^\\pi$ to Stage 1: $\\mathbf{B}^{\\pi'_D}(\\mathbf{V}^\\pi)$\n",
    "4. Stage 2: VF $(\\mathbf{B}^{\\pi'_D})^2(\\mathbf{V}^\\pi)$ means execute improved policy $\\pi'_D$ for the 1st and 2nd time step and execute policy $\\pi$ for the rest of time steps\n",
    "5. improved the VF from Stage 1: $\\mathbf{B}^{\\pi'_D}(\\mathbf{V}^\\pi)$ to Stage 2: $(\\mathbf{B}^{\\pi'_D})^2(\\mathbf{V}^\\pi)$\n",
    "6. Each step applies a policy $\\pi'_D$ instead of $\\pi$ an extra tme step\n",
    "\n",
    "# Policy Iteration algorithm gist\n",
    "1. Policy Improvement THeorem says:\n",
    "    1. start with a value function $\\mathbf{V}^\\pi$ (for policy $\\pi$)\n",
    "    2. Perform a \"greedy policy improvement: to create policy $pi'_D = G(\\mathbf{V}^\\pi)$\n",
    "    3. Perform Policy evaluation(for policy $\\pi'_D$) starting with VF $\\mathbf{V}^\\pi$\n",
    "    4. This result in VF of $\\mathbf{V}^{\\pi'_D}$ which is greater than VF $ \\mathbf{V}^\\pi$\n",
    "2. We can repeat the above starting with an improved policy $\\pi''_D$ and improved VF $\\mathbf{V}^{\\pi''_D}$\n",
    "3. we repeat till there's no further Improvement\n",
    "\n",
    "## Policy Iteration algorithm\n",
    "1. Start with any Value Function $V_0\\in\\mathbb{R}^m$\n",
    "2. Iterate over $j=0,1,2,\\dots$ calculate in each iteration\n",
    "\n",
    "\\begin{equation}\n",
    "\\text{Deterministic Policy }\\pi_{j+1}=G(V_j)\\\\\n",
    "\\text{Value Function }\\mathbf{V}_{j+1} = \\lim_{i\\rightarrow\\infty}((\\mathbf{B}^{\\pi_{j+1}})^i(\\mathbf{V}_j))\n",
    "\\end{equation}\n",
    "\n",
    "3. Stop when $\\mathbf{V}_{j+1}$ is closed to $\\mathbf{V}_{j}$, i.e. $d(\\mathbf{V}_{j+1},\\mathbf{V}_{j}$ is small enough.  \n",
    "At termination: $\\mathbf{V}_{j}=(\\mathbf{B}^{G(\\mathbf{V}_j)})^i(\\mathbf{V}_j) = \\mathbf{V}_{j+1}$\n",
    "\n",
    "## Policy Iteration Convergence Theorem\n",
    "For a Finite MDP with $|\\mathcal{N}|=m$ and $\\gamma <1$, Policy Iteration algorithm converges to the Optimal Value Function $\\mathbf{V}^∗\\in\\mathbb{R}^m$ along with a Deterministic Optimal Policy $ \\pi^∗_D:N →A$, no matter which Value Function $V_0∈\\mathbb{R}^m$ we start the algorithm with."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Value Iteration\n",
    "\n",
    "## Bellman Optimality Operator\n",
    "$B^\\pi$ is motivated by MDP bellman policy equation, similarly we can have $B^*$ which is motivated by MDP bellman optimality Equation. Bellman Optimality Operator $B^*:\\mathbb{R}^m \\rightarrow \\mathbb{R}^m$\n",
    "\\begin{equation}\n",
    "B^*(\\mathbf{V})(s) = \\max_{a\\in\\mathcal{A}}\\{ \\mathcal{R}(s,a) +\\gamma \\sum_{s'\\in\\mathcal{N}}\\mathcal{P}(s,a,s')\\cdot \\mathbf{V}(s')\\}\n",
    "\\end{equation}\n",
    "Note the G is used to find the best action, while B is to find the best value given the best action, so the best action is provided by $\\pi_D$. and\n",
    "\n",
    "\\begin{equation}\n",
    "B^{G(V^\\pi)}(V) = B^*(V) \\text{ for all $V\\in\\mathbb{R}^m$}\n",
    "\\end{equation}\n",
    "\n",
    "Express V^* in the original form\n",
    "\\begin{equation}\n",
    "V^*(s) = \\max_{a\\in\\mathcal{A}}\\{ \\mathcal{R}(s,a) +\\gamma \\sum_{s'\\in\\mathcal{N}}\\mathcal{P}(s,a,s')\\cdot \\mathbf{V}(s')\\} = B^*(V^*)\n",
    "\\end{equation}\n",
    "\n",
    "We can prove that it's a contraction and use Banach Fixed-point theorem again.\n",
    "\n",
    "## Value Iteration Convergence Theorem\n",
    "For a Finite MDP with $|N|=m$ and $\\gamma <1$, if $V^∗\\in\\mathbb{R}^m$ is the Optimal Value Function, then $V^∗$ is the unique Fixed-Point of the Bellman Optimality Operator $B^*:\\mathbb{R}^m \\rightarrow \\mathbb{R}^m$, and\n",
    "\n",
    "\\begin{equation}\n",
    "\\lim_{i\\rightarrow \\infty}(B^*)^i(V_0)\\rightarrow V^* \\text{ for all starting Value Function $V_0 \\in \\mathbb{R}^m$}\n",
    "\\end{equation}\n",
    "\n",
    "## Value Iteration Algorithm\n",
    "1. Start with any Value Function $V_0\\in \\mathbb{R}^m$\n",
    "2. Iterating over $i=0,1,2,\\dots$ calculate each iteration:\n",
    "\\begin{equation}\n",
    "V_{i+1}(s) = B^*(V_i)(s) \\text{ for all $s\\in\\mathcal{N}$}\n",
    "\\end{equation}\n",
    "\n",
    "3. Stop when $d(V_i,V_{i+1})$ is small enough\n",
    "\n",
    "Running time for each iteration of Value Iteration is $O(m^2k)$ where $|\\mathcal{N}|=m$ and $|\\mathcal{A}|=k$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
