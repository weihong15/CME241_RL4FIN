{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 1\n",
    "\n",
    "In the question, We were given $\\mathcal{G}, \\mathcal{T}, \\mathcal{B}$, we will use this to formulate our state space\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathcal{N} = \\mathcal{G} -\\mathcal{B} - \\mathcal{T} \\\\\n",
    "\\mathcal{S} = \\mathcal{N} \\cup \\mathcal{T} \\\\\n",
    "\\mathcal{A} = \\{L,R,U,D\\} \n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define an indicator function s.t. \n",
    "\\begin{equation}\n",
    " \\mathcal{I}((x,y)) = \n",
    "  \\begin{cases} \n",
    "      \\hfill 1    \\hfill & \\text{ if $(x,y)\\in \\mathcal{S}$} \\\\\n",
    "      \\hfill 0    \\hfill & \\text{ o.w.}\n",
    "  \\end{cases}\n",
    "\\end{equation}\n",
    "\n",
    "We define the state transition probability $\\mathcal{P}((x,y),a,(x',y'))$ for action $ a = L$\n",
    "\n",
    "\\begin{equation}\n",
    " \\mathcal{P}((x,y),L,(x',y')) =\n",
    "  \\begin{cases} \n",
    "      \\hfill 1    \\hfill & \\text{ if $x'=x, y'= y-1, (x',y')\\in \\mathcal{T}$} \\\\\n",
    "      \\hfill p_2 \\cdot \\mathcal{I}((x',y'))    \\hfill & \\text{ if $x'=x-1, y'= y-1, (x',y')\\in \\mathcal{S}, (x,y-1)\\in \\mathcal{N}$} \\\\\n",
    "      \\hfill p_1 \\cdot \\mathcal{I}((x',y'))   \\hfill & \\text{ if $x'=x+1, y'= y-1, (x',y')\\in \\mathcal{S}, (x,y-1)\\in \\mathcal{N}$} \\\\\n",
    "      \\hfill 1-p_1\\cdot \\mathcal{I}((x+1,y-1))-p_2 \\cdot \\mathcal{I}((x-1,y-1))   \\hfill & \\text{ if $x'=x, y'= y-1, (x',y')\\in \\mathcal{N}$} \\\\\n",
    "      \\hfill 0 \\hfill & \\text{o.w.}\n",
    "  \\end{cases}\n",
    "\\end{equation}\n",
    "\n",
    "Likewise we can define the state transition probability for the rest of action, R,U,D\n",
    "\\begin{equation}\n",
    " \\mathcal{P}((x,y),R,(x',y')) =\n",
    "  \\begin{cases} \n",
    "      \\hfill 1    \\hfill & \\text{ if $x'=x, y'= y+1, (x',y')\\in \\mathcal{T}$} \\\\\n",
    "      \\hfill p_2 \\cdot \\mathcal{I}((x',y'))    \\hfill & \\text{ if $x'=x-1, y'= y+1, (x',y')\\in \\mathcal{S}, (x,y+1)\\in \\mathcal{N}$} \\\\\n",
    "      \\hfill p_1 \\cdot \\mathcal{I}((x',y'))   \\hfill & \\text{ if $x'=x+1, y'= y+1, (x',y')\\in \\mathcal{S}, (x,y+1)\\in \\mathcal{N}$} \\\\\n",
    "      \\hfill 1-p_1\\cdot \\mathcal{I}((x+1,y+1))-p_2 \\cdot \\mathcal{I}((x-1,y+1))   \\hfill & \\text{ if $x'=x, y'= y+1, (x',y')\\in \\mathcal{N}$} \\\\\n",
    "      \\hfill 0 \\hfill & \\text{o.w.}\n",
    "  \\end{cases}\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    " \\mathcal{P}((x,y),U,(x',y')) =\n",
    "  \\begin{cases} \n",
    "      \\hfill 1    \\hfill & \\text{ if $x'=x, y'= y-1, (x',y')\\in \\mathcal{T}$} \\\\\n",
    "      \\hfill p_2 \\cdot \\mathcal{I}((x',y'))    \\hfill & \\text{ if $x'=x, y'= y, (x',y')\\in \\mathcal{S}, (x+1,y)\\in \\mathcal{N}$} \\\\\n",
    "      \\hfill p_1 \\cdot \\mathcal{I}((x',y'))   \\hfill & \\text{ if $x'=x+2, y'= y, (x',y')\\in \\mathcal{S}, (x+1,y)\\in \\mathcal{N}$} \\\\\n",
    "      \\hfill 1-p_1\\cdot \\mathcal{I}((x+2,y))-p_2 \\cdot \\mathcal{I}((x,y))   \\hfill & \\text{ if $x'=x, y'= y, (x',y')\\in \\mathcal{N}$} \\\\\n",
    "      \\hfill 0 \\hfill & \\text{o.w.}\n",
    "  \\end{cases}\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    " \\mathcal{P}((x,y),D,(x',y')) =\n",
    "  \\begin{cases} \n",
    "      \\hfill 1    \\hfill & \\text{ if $x'=x, y'= y-1, (x',y')\\in \\mathcal{T}$} \\\\\n",
    "      \\hfill p_2 \\cdot \\mathcal{I}((x',y'))    \\hfill & \\text{ if $x'=x-2, y'= y, (x',y')\\in \\mathcal{S}, (x-1,y)\\in \\mathcal{N}$} \\\\\n",
    "      \\hfill p_1 \\cdot \\mathcal{I}((x',y'))   \\hfill & \\text{ if $x'=x, y'= y, (x',y')\\in \\mathcal{S}, (x-1,y)\\in \\mathcal{N}$} \\\\\n",
    "      \\hfill 1-p_1\\cdot \\mathcal{I}((x,y))-p_2 \\cdot \\mathcal{I}((x-2,y))   \\hfill & \\text{ if $x'=x, y'= y, (x',y')\\in \\mathcal{N}$} \\\\\n",
    "      \\hfill 0 \\hfill & \\text{o.w.}\n",
    "  \\end{cases}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will denote the random bernoulli function with parameter p as $Be(p)$. where it is 1 with probability p and 0 with probability 1-p\n",
    "\n",
    "\\begin{equation}\n",
    " \\mathcal{R}_T((x,y),L,(x',y')) =\n",
    "  \\begin{cases} \n",
    "      \\hfill -1    \\hfill & \\text{ if $x'=x, y'= y-1, (x',y')\\in \\mathcal{T}$} \\\\\n",
    "      \\hfill -1    \\hfill & \\text{ if $x'=x-1, y'= y-1, (x',y')\\in \\mathcal{S}, (x,y-1)\\in \\mathcal{N}$} \\\\\n",
    "      \\hfill -1    \\hfill & \\text{ if $x'=x+1, y'= y-1, (x',y')\\in \\mathcal{S}, (x,y-1)\\in \\mathcal{N}$} \\\\\n",
    "      \\hfill -1-b(Be(p_1))   \\hfill & \\text{ if $x'=x, y'= y-1, (x+1,y-1)\\in \\mathcal{B} \\cap (x-1,y-1)\\in \\mathcal{S}$} \\\\\n",
    "      \\hfill -1-b(Be(p_2))   \\hfill & \\text{ if $x'=x, y'= y-1, (x-1,y-1)\\in \\mathcal{B} \\cap (x+1,y-1)\\in \\mathcal{S}$} \\\\\n",
    "      \\hfill -1-b(Be(p_1+p_2))   \\hfill & \\text{ if $x'=x, y'= y-1, (x+1,y-1)\\in \\mathcal{B} \\cap (x-1,y-1)\\in \\mathcal{B}$} \\\\\n",
    "      \\hfill 0 \\hfill & \\text{o.w.}\n",
    "  \\end{cases}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    " \\mathcal{R}_T((x,y),R,(x',y')) =\n",
    "  \\begin{cases} \n",
    "      \\hfill -1    \\hfill & \\text{ if $x'=x, y'= y+1, (x',y')\\in \\mathcal{T}$} \\\\\n",
    "      \\hfill -1    \\hfill & \\text{ if $x'=x-1, y'= y+1, (x',y')\\in \\mathcal{S}, (x,y+1)\\in \\mathcal{N}$} \\\\\n",
    "      \\hfill -1    \\hfill & \\text{ if $x'=x+1, y'= y+1, (x',y')\\in \\mathcal{S}, (x,y+1)\\in \\mathcal{N}$} \\\\\n",
    "      \\hfill -1-b(Be(p_1))   \\hfill & \\text{ if $x'=x, y'= y+1, (x+1,y+1)\\in \\mathcal{B} \\cap (x-1,y+1)\\in \\mathcal{S}$} \\\\\n",
    "      \\hfill -1-b(Be(p_2))   \\hfill & \\text{ if $x'=x, y'= y+1, (x-1,y+1)\\in \\mathcal{B} \\cap (x+1,y+1)\\in \\mathcal{S}$} \\\\\n",
    "      \\hfill -1-b(Be(p_1+p_2))   \\hfill & \\text{ if $x'=x, y'= y+1, (x+1,y+1)\\in \\mathcal{B} \\cap (x-1,y+1)\\in \\mathcal{B}$} \\\\\n",
    "      \\hfill 0 \\hfill & \\text{ o.w.}\n",
    "  \\end{cases}\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    " \\mathcal{R}_T((x,y),U,(x',y')) =\n",
    "  \\begin{cases} \n",
    "      \\hfill -1    \\hfill & \\text{ if $x'=x+1, y'= y, (x',y')\\in \\mathcal{T}$} \\\\\n",
    "      \\hfill -1    \\hfill & \\text{ if $x'=x+2, y'= y, (x',y')\\in \\mathcal{S}, (x+1,y)\\in \\mathcal{N}$} \\\\\n",
    "      \\hfill -1    \\hfill & \\text{ if $x'=x, y'= y, (x',y')\\in \\mathcal{S}, (x+1,y)\\in \\mathcal{N}$} \\\\\n",
    "      \\hfill -1-b(Be(p_1))   \\hfill & \\text{ if $x'=x+1, y'= y, (x+2,y)\\in \\mathcal{B} \\cap (x+1,y)\\in \\mathcal{N}$} \\\\\n",
    "      \\hfill 0 \\hfill & \\text{ o.w.}\n",
    "  \\end{cases}\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    " \\mathcal{R}_T((x,y),D,(x',y')) =\n",
    "  \\begin{cases} \n",
    "      \\hfill -1    \\hfill & \\text{ if $x'=x-1, y'= y, (x',y')\\in \\mathcal{T}$} \\\\\n",
    "      \\hfill -1    \\hfill & \\text{ if $x'=x, y'= y, (x',y')\\in \\mathcal{S}, (x-1,y)\\in \\mathcal{N}$} \\\\\n",
    "      \\hfill -1    \\hfill & \\text{ if $x'=x-2, y'= y, (x',y')\\in \\mathcal{S}, (x,y)\\in \\mathcal{N}$} \\\\\n",
    "      \\hfill -1-b(Be(p_2))   \\hfill & \\text{ if $x'=x-1, y'= y, (x-2,y)\\in \\mathcal{B} \\cap (x-1,y)\\in \\mathcal{N}$} \\\\\n",
    "      \\hfill 0 \\hfill & \\text{ o.w.}\n",
    "  \\end{cases}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "import random\n",
    "from typing import Tuple, Sequence, Set, Mapping, Dict, Callable, Optional\n",
    "from dataclasses import dataclass\n",
    "from operator import itemgetter\n",
    "from rl.distribution import Categorical, Choose, Constant\n",
    "from rl.markov_decision_process import FiniteMarkovDecisionProcess\n",
    "from rl.markov_decision_process import StateActionMapping\n",
    "from rl.markov_decision_process import FinitePolicy\n",
    "from rl.dynamic_programming import value_iteration_result, V\n",
    "\n",
    "'''\n",
    "Cell specifies (row, column) coordinate\n",
    "'''\n",
    "Cell = Tuple[int, int]\n",
    "CellSet = Set[Cell]\n",
    "Move = Tuple[int, int]\n",
    "'''\n",
    "WindSpec specifies a random vectical wind for each column.\n",
    "Each random vertical wind is specified by a (p1, p2) pair\n",
    "where p1 specifies probability of Downward Wind (could take you\n",
    "one step lower in row coordinate unless prevented by a block or\n",
    "boundary) and p2 specifies probability of Upward Wind (could take\n",
    "you onw step higher in column coordinate unless prevented by a\n",
    "block or boundary). If one bumps against a block or boundary, one\n",
    "incurs a bump cost and doesn't move. The remaining probability\n",
    "1- p1 - p2 corresponds to No Wind.\n",
    "'''\n",
    "WindSpec = Sequence[Tuple[float, float]]\n",
    "\n",
    "possible_moves: Mapping[Move, str] = {\n",
    "    (-1, 0): 'D',\n",
    "    (1, 0): 'U',\n",
    "    (0, -1): 'L',\n",
    "    (0, 1): 'R'\n",
    "}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass(frozen=True)\n",
    "class WindyGrid:\n",
    "\n",
    "    rows: int  # number of grid rows\n",
    "    columns: int  # number of grid columns\n",
    "    blocks: CellSet  # coordinates of block cells\n",
    "    terminals: CellSet  # coordinates of goal cells\n",
    "    wind: WindSpec  # spec of vertical random wind for the columns\n",
    "    bump_cost: float  # cost of bumping against block or boundary\n",
    "\n",
    "    def validate_spec(self) -> bool:\n",
    "        b1 = self.rows >= 2\n",
    "        b2 = self.columns >= 2\n",
    "        b3 = all(0 <= r < self.rows and 0 <= c < self.columns\n",
    "                 for r, c in self.blocks)\n",
    "        b4 = len(self.terminals) >= 1\n",
    "        b5 = all(0 <= r < self.rows and 0 <= c < self.columns and\n",
    "                 (r, c) not in self.blocks for r, c in self.terminals)\n",
    "        b6 = len(self.wind) == self.columns\n",
    "        b7 = all(0. <= p1 <= 1. and 0. <= p2 <= 1. and p1 + p2 <= 1.\n",
    "                 for p1, p2 in self.wind)\n",
    "        b8 = self.bump_cost > 0.\n",
    "        return all([b1, b2, b3, b4, b5, b6, b7, b8])\n",
    "\n",
    "    def print_wind_and_bumps(self) -> None:\n",
    "        for i, (d, u) in enumerate(self.wind):\n",
    "            print(f\"Column {i:d}: Down Prob = {d:.2f}, Up Prob = {u:.2f}\")\n",
    "        print(f\"Bump Cost = {self.bump_cost:.2f}\")\n",
    "        print()\n",
    "\n",
    "    @staticmethod\n",
    "    def add_tuples(a: Cell, b: Cell) -> Cell:\n",
    "        return a[0] + b[0], a[1] + b[1]\n",
    "\n",
    "    def is_valid_state(self, cell: Cell) -> bool:\n",
    "        '''\n",
    "        checks if a cell is a valid state of the MDP\n",
    "        '''\n",
    "        return 0 <= cell[0] < self.rows and 0 <= cell[1] < self.columns \\\n",
    "            and cell not in self.blocks\n",
    "\n",
    "    def get_all_nt_states(self) -> CellSet:\n",
    "        '''\n",
    "        returns all the non-terminal states\n",
    "        '''\n",
    "        return {(i, j) for i in range(self.rows) for j in range(self.columns)\n",
    "                if (i, j) not in set.union(self.blocks, self.terminals)}\n",
    "\n",
    "    def get_actions_and_next_states(self, nt_state: Cell) \\\n",
    "            -> Set[Tuple[Move, Cell]]:\n",
    "        '''\n",
    "        given a non-terminal state, returns the set of all possible\n",
    "        (action, next_state) pairs\n",
    "        '''\n",
    "        temp: Set[Tuple[Move, Cell]] = {(a, WindyGrid.add_tuples(nt_state, a))\n",
    "                                        for a in possible_moves}\n",
    "        return {(a, s) for a, s in temp if self.is_valid_state(s)}\n",
    "\n",
    "    def get_transition_probabilities(self, nt_state: Cell) \\\n",
    "            -> Mapping[Move, Categorical[Tuple[Cell, float]]]:\n",
    "        '''\n",
    "        given a non-terminal state, return a dictionary whose\n",
    "        keys are the valid actions (moves) from the given state\n",
    "        and the corresponding values are the associated probabilities\n",
    "        (following that move) of the (next_state, reward) pairs.\n",
    "        The probabilities are determined from the wind probabilities\n",
    "        of the column one is in after the move. Note that if one moves\n",
    "        to a goal cell (terminal state), then one ends up in that\n",
    "        goal cell with 100% probability (i.e., no wind exposure in a\n",
    "        goal cell).\n",
    "        '''\n",
    "        d: Dict[Move, Categorical[Tuple[Cell, float]]] = {}\n",
    "        for a, (r, c) in self.get_actions_and_next_states(nt_state):\n",
    "            if (r, c) in self.terminals:\n",
    "                d[a] = Categorical({((r, c), -1.): 1.})\n",
    "            else:\n",
    "                '''\n",
    "                write your code here\n",
    "                '''\n",
    "                up = WindyGrid.add_tuples((1,0), (r,c))\n",
    "                down = WindyGrid.add_tuples((-1,0), (r,c))\n",
    "                validUp = self.is_valid_state(up)\n",
    "                validDown = self.is_valid_state(down)\n",
    "                upP = self.wind[c][0]\n",
    "                downP = self.wind[c][1]\n",
    "                if(validUp and validDown):\n",
    "                    # will nv bump into wall    \n",
    "                    d[a] = Categorical({((r, c), -1.): 1.-upP -downP,\n",
    "                                        (WindyGrid.add_tuples((1,0), (r,c)),-1.): upP,\n",
    "                                        (WindyGrid.add_tuples((-1,0), (r,c)),-1.): downP\n",
    "                                       })\n",
    "                elif validUp:\n",
    "                    # might bump downwards\n",
    "                    d[a] = Categorical({((r, c), -1.): 1.-upP -downP,\n",
    "                                        (WindyGrid.add_tuples((1,0), (r,c)),-1.): upP,\n",
    "                                        ((r,c),-1.-self.bump_cost): downP\n",
    "                                       })\n",
    "                elif validDown:\n",
    "                    # might bump upwards\n",
    "                    d[a] = Categorical({((r, c), -1.): 1.-upP -downP,\n",
    "                                        ((r,c),-1.-self.bump_cost): upP,\n",
    "                                        (WindyGrid.add_tuples((-1,0), (r,c)),-1.): downP\n",
    "                                       })\n",
    "                else:\n",
    "                    # might bump up or down    \n",
    "                    d[a] = Categorical({((r, c), -1.): 1.-upP -downP,\n",
    "                                        ((r,c),-1.-self.bump_cost): upP+downP\n",
    "                                       })\n",
    "                \n",
    "                \n",
    "        return d\n",
    "\n",
    "    def get_finite_mdp(self) -> FiniteMarkovDecisionProcess[Cell, Move]:\n",
    "        '''\n",
    "        returns the FiniteMarkovDecision object for this windy grid problem\n",
    "        '''\n",
    "        d1: StateActionMapping[Cell, Move] = \\\n",
    "            {s: self.get_transition_probabilities(s) for s in\n",
    "             self.get_all_nt_states()}\n",
    "        d2: StateActionMapping[Cell, Move] = {s: None for s in self.terminals}\n",
    "        return FiniteMarkovDecisionProcess({**d1, **d2})\n",
    "\n",
    "    def get_vi_vf_and_policy(self) -> Tuple[V[Cell], FinitePolicy[Cell, Move]]:\n",
    "        '''\n",
    "        Performs the Value Iteration DP algorithm returning the\n",
    "        Optimal Value Function (as a V[Cell]) and the Optimal Policy\n",
    "        (as a FinitePolicy[Cell, Move])\n",
    "        '''\n",
    "        return value_iteration_result(self.get_finite_mdp(), gamma=1.)\n",
    "\n",
    "    @staticmethod\n",
    "    def epsilon_greedy_action(\n",
    "        nt_state: Cell,\n",
    "        q: Mapping[Cell, Mapping[Move, float]],\n",
    "        epsilon: float\n",
    "    ) -> Move:\n",
    "        '''\n",
    "        given a non-terminal state, a Q-Value Function (in the form of a\n",
    "        {state: {action: Expected Return}} dictionary) and epislon, return\n",
    "        an action sampled from the probability distribution implied by an\n",
    "        epsilon-greedy policy that is derived from the Q-Value Function.\n",
    "        '''\n",
    "        action_values: Mapping[Move, float] = q[nt_state]\n",
    "        greedy_action: Move = max(action_values.items(), key=itemgetter(1))[0]\n",
    "        return Categorical(\n",
    "            {a: epsilon / len(action_values) +\n",
    "             (1 - epsilon if a == greedy_action else 0.)\n",
    "             for a in action_values}\n",
    "        ).sample()\n",
    "\n",
    "    def get_states_actions_dict(self) -> Mapping[Cell, Optional[Set[Move]]]:\n",
    "        '''\n",
    "        Returns a dictionary whose keys are the states and the corresponding\n",
    "        values are the set of actions for the state (if the key is a\n",
    "        non-terminal state) or is None if the state is a terminal state.\n",
    "        '''\n",
    "        d1: Mapping[Cell, Optional[Set[Move]]] = \\\n",
    "            {s: {a for a, _ in self.get_actions_and_next_states(s)}\n",
    "             for s in self.get_all_nt_states()}\n",
    "        d2: Mapping[Cell, Optional[Set[Move]]] = \\\n",
    "            {s: None for s in self.terminals}\n",
    "        return {**d1, **d2}\n",
    "\n",
    "    def get_sarsa_vf_and_policy(\n",
    "        self,\n",
    "        states_actions_dict: Mapping[Cell, Optional[Set[Move]]],\n",
    "        sample_func: Callable[[Cell, Move], Tuple[Cell, float]],\n",
    "        episodes: int = 10000,\n",
    "        step_size: float = 0.01\n",
    "    ) -> Tuple[V[Cell], FinitePolicy[Cell, Move]]:\n",
    "        '''\n",
    "        states_actions_dict gives us the set of possible moves from\n",
    "        a non-block cell.\n",
    "        sample_func is a function with two inputs: state and action,\n",
    "        and with output as a sampled pair of (next_state, reward).\n",
    "        '''\n",
    "        q: Dict[Cell, Dict[Move, float]] = \\\n",
    "            {s: {a: 0. for a in actions} for s, actions in\n",
    "             states_actions_dict.items() if actions is not None}\n",
    "        nt_states: CellSet = {s for s in q}\n",
    "        uniform_states: Choose[Cell] = Choose(nt_states)\n",
    "        for episode_num in range(episodes):\n",
    "            epsilon: float = 1.0 / (episode_num + 1)\n",
    "            state: Cell = uniform_states.sample()\n",
    "            \n",
    "            '''\n",
    "            write your code here\n",
    "            update the dictionary q initialized above according\n",
    "            to the SARSA algorithm's Q-Value Function updates.\n",
    "            '''\n",
    "            ## q(s,a) = q(s,a) + alpha(r_{t+1} + gamma q(s_{t+1},a_{t+1}) - q(s,a))\n",
    "            greedy1: bool = random.random()<(1-epsilon)\n",
    "            greedy2: bool = random.random()<(1-epsilon)\n",
    "            if greedy1:\n",
    "                ##greedy policy\n",
    "                act: Move = max(q[state],key= q[state].get)\n",
    "            else:\n",
    "                ## exploration\n",
    "                act = random.choice(list(states_actions_dict[state]))\n",
    "                ## above doesn't work for set. \n",
    "            nextState,reward = sample_func(state,act)\n",
    "            ## check if terminal state\n",
    "            if nextState not in nt_states:\n",
    "                q[state][act] += step_size *(reward + 0 - q[state][act])\n",
    "                continue\n",
    "            if greedy2:\n",
    "                ## greedy policy\n",
    "                act2: Move = max(q[nextState],key= q[nextState].get)\n",
    "            else:\n",
    "                ## exploration\n",
    "                act2 = random.choice(list(states_actions_dict[nextState]))\n",
    "            ### ask about learning rate, and step size\n",
    "            ## ask sven, it's equivalent\n",
    "            ## alpha = step_size\n",
    "            q[state][act] += step_size *(reward + q[nextState][act2]-q[state][act])\n",
    "            \n",
    "        vf_dict: V[Cell] = {s: max(d.values()) for s, d in q.items()}\n",
    "        policy: FinitePolicy[Cell, Move] = FinitePolicy(\n",
    "            {s: Constant(max(d.items(), key=itemgetter(1))[0])\n",
    "             for s, d in q.items()}\n",
    "        )\n",
    "        return (vf_dict, policy)\n",
    "\n",
    "    def get_q_learning_vf_and_policy(\n",
    "        self,\n",
    "        states_actions_dict: Mapping[Cell, Optional[Set[Move]]],\n",
    "        sample_func: Callable[[Cell, Move], Tuple[Cell, float]],\n",
    "        episodes: int = 10000,\n",
    "        step_size: float = 0.01,\n",
    "        epsilon: float = 0.1\n",
    "    ) -> Tuple[V[Cell], FinitePolicy[Cell, Move]]:\n",
    "        '''\n",
    "        states_actions_dict gives us the set of possible moves from\n",
    "        a non-block cell.\n",
    "        sample_func is a function with two inputs: state and action,\n",
    "        and with output as a sampled pair of (next_state, reward).\n",
    "        '''\n",
    "        q: Dict[Cell, Dict[Move, float]] = \\\n",
    "            {s: {a: 0. for a in actions} for s, actions in\n",
    "             states_actions_dict.items() if actions is not None}\n",
    "        nt_states: CellSet = {s for s in q}\n",
    "        uniform_states: Choose[Cell] = Choose(nt_states)\n",
    "        for episode_num in range(episodes):\n",
    "            state: Cell = uniform_states.sample()\n",
    "            epsilon: float = 1.0 / (episode_num + 1)\n",
    "            '''\n",
    "            write your code here\n",
    "            update the dictionary q initialized above according\n",
    "            to the Q-learning algorithm's Q-Value Function updates.\n",
    "            '''\n",
    "            greedy1: bool = random.random()<(1-epsilon)\n",
    "            if greedy1:\n",
    "                ##greedy policy\n",
    "                act: Move = max(q[state],key= q[state].get)\n",
    "            else:\n",
    "                ## exploration\n",
    "                act = random.choice(list(states_actions_dict[state]))\n",
    "            nextState,reward = sample_func(state,act)\n",
    "            \n",
    "            \n",
    "            ## check if terminal state\n",
    "            \n",
    "            if nextState not in nt_states:\n",
    "                q[state][act] += step_size *(reward + 0 - q[state][act])\n",
    "                continue\n",
    "            act2: Move = max(q[nextState],key= q[nextState].get)\n",
    "\n",
    "            ### ask about learning rate, and step size\n",
    "            ## small default value for step_size, seems like will not learn quickly\n",
    "            q[state][act] += step_size *(reward + q[nextState][act2]-q[state][act])\n",
    "        vf_dict: V[Cell] = {s: max(d.values()) for s, d in q.items()}\n",
    "        policy: FinitePolicy[Cell, Move] = FinitePolicy(\n",
    "            {s: Constant(max(d.items(), key=itemgetter(1))[0])\n",
    "             for s, d in q.items()}\n",
    "        )\n",
    "        return (vf_dict, policy)\n",
    "\n",
    "    def print_vf_and_policy(\n",
    "        self,\n",
    "        vf_dict: V[Cell],\n",
    "        policy: FinitePolicy[Cell, Move]\n",
    "    ) -> None:\n",
    "        display = \"%5.2f\"\n",
    "        display1 = \"%5d\"\n",
    "        vf_full_dict = {\n",
    "            **{s: display % -v for s, v in vf_dict.items()},\n",
    "            **{s: display % 0.0 for s in self.terminals},\n",
    "            **{s: 'X' * 5 for s in self.blocks}\n",
    "        }\n",
    "        print(\"   \" + \" \".join([display1 % j for j in range(self.columns)]))\n",
    "        for i in range(self.rows - 1, -1, -1):\n",
    "            print(\"%2d \" % i + \" \".join(vf_full_dict[(i, j)]\n",
    "                                        for j in range(self.columns)))\n",
    "        print()\n",
    "        pol_full_dict = {\n",
    "            **{s: possible_moves[policy.act(s).value]\n",
    "               for s in self.get_all_nt_states()},\n",
    "            **{s: 'T' for s in self.terminals},\n",
    "            **{s: 'X' for s in self.blocks}\n",
    "        }\n",
    "        print(\"   \" + \" \".join([\"%2d\" % j for j in range(self.columns)]))\n",
    "        for i in range(self.rows - 1, -1, -1):\n",
    "            print(\"%2d  \" % i + \"  \".join(pol_full_dict[(i, j)]\n",
    "                                          for j in range(self.columns)))\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column 0: Down Prob = 0.00, Up Prob = 0.90\n",
      "Column 1: Down Prob = 0.00, Up Prob = 0.80\n",
      "Column 2: Down Prob = 0.70, Up Prob = 0.00\n",
      "Column 3: Down Prob = 0.80, Up Prob = 0.00\n",
      "Column 4: Down Prob = 0.90, Up Prob = 0.00\n",
      "Bump Cost = 4.00\n",
      "\n",
      "iterations made =  125\n",
      "Value Iteration\n",
      "\n",
      "       0     1     2     3     4\n",
      " 4 XXXXX  8.95  9.01  5.60  1.00\n",
      " 3 XXXXX  9.01  5.68  1.00  0.00\n",
      " 2  9.87  7.68  9.01 XXXXX  1.00\n",
      " 1 13.37  9.17  6.21  2.01  1.10\n",
      " 0 23.37 XXXXX XXXXX  6.21 XXXXX\n",
      "\n",
      "    0  1  2  3  4\n",
      " 4  X  D  D  R  D\n",
      " 3  X  R  R  R  T\n",
      " 2  R  R  U  X  U\n",
      " 1  R  R  R  R  U\n",
      " 0  U  X  X  U  X\n",
      "\n",
      "SARSA\n",
      "\n",
      "       0     1     2     3     4\n",
      " 4 XXXXX  9.01  8.93  5.65  1.00\n",
      " 3 XXXXX  9.03  5.86  1.00  0.00\n",
      " 2 10.07  7.92  9.09 XXXXX  1.00\n",
      " 1 13.57  9.29  6.15  2.01  1.09\n",
      " 0 22.73 XXXXX XXXXX  6.09 XXXXX\n",
      "\n",
      "    0  1  2  3  4\n",
      " 4  X  D  D  R  D\n",
      " 3  X  R  R  R  T\n",
      " 2  R  R  U  X  U\n",
      " 1  R  R  R  R  U\n",
      " 0  U  X  X  U  X\n",
      "\n",
      "Q-Learning\n",
      "\n",
      "       0     1     2     3     4\n",
      " 4 XXXXX  6.72  6.49  5.39  1.00\n",
      " 3 XXXXX  6.22  5.25  1.00  0.00\n",
      " 2  7.05  6.12  6.20 XXXXX  1.00\n",
      " 1  9.50  6.59  6.10  2.01  1.07\n",
      " 0 12.94 XXXXX XXXXX  6.28 XXXXX\n",
      "\n",
      "    0  1  2  3  4\n",
      " 4  X  D  D  R  D\n",
      " 3  X  R  R  R  T\n",
      " 2  R  R  D  X  U\n",
      " 1  R  R  R  R  U\n",
      " 0  U  X  X  U  X\n",
      "\n"
     ]
    }
   ],
   "source": [
    "wg = WindyGrid(\n",
    "    rows=5,\n",
    "    columns=5,\n",
    "    blocks={(0, 1), (0, 2), (0, 4), (2, 3), (3, 0), (4, 0)},\n",
    "    terminals={(3, 4)},\n",
    "    wind=[(0., 0.9), (0.0, 0.8), (0.7, 0.0), (0.8, 0.0), (0.9, 0.0)],\n",
    "    bump_cost=4.0\n",
    ")\n",
    "valid = wg.validate_spec()\n",
    "if valid:\n",
    "    wg.print_wind_and_bumps()\n",
    "    vi_vf_dict, vi_policy = wg.get_vi_vf_and_policy()\n",
    "    print(\"Value Iteration\\n\")\n",
    "    wg.print_vf_and_policy(\n",
    "        vf_dict=vi_vf_dict,\n",
    "        policy=vi_policy\n",
    "    )\n",
    "    mdp: FiniteMarkovDecisionProcess[Cell, Move] = wg.get_finite_mdp()\n",
    "\n",
    "    def sample_func(state: Cell, action: Move) -> Tuple[Cell, float]:\n",
    "        return mdp.step(state, action).sample()\n",
    "\n",
    "    sarsa_vf_dict, sarsa_policy = wg.get_sarsa_vf_and_policy(\n",
    "        states_actions_dict=wg.get_states_actions_dict(),\n",
    "        sample_func=sample_func,\n",
    "        episodes=100000,\n",
    "        step_size=0.01\n",
    "    )\n",
    "    print(\"SARSA\\n\")\n",
    "    wg.print_vf_and_policy(\n",
    "        vf_dict=sarsa_vf_dict,\n",
    "        policy=sarsa_policy\n",
    "    )\n",
    "\n",
    "    ql_vf_dict, ql_policy = wg.get_q_learning_vf_and_policy(\n",
    "        states_actions_dict=wg.get_states_actions_dict(),\n",
    "        sample_func=sample_func,\n",
    "        episodes=30000,\n",
    "        step_size=0.01,\n",
    "        epsilon=0.1\n",
    "    )\n",
    "    print(\"Q-Learning\\n\")\n",
    "    wg.print_vf_and_policy(\n",
    "        vf_dict=ql_vf_dict,\n",
    "        policy=ql_policy\n",
    "    )\n",
    "\n",
    "else:\n",
    "    print(\"Invalid Spec of Windy Grid\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
