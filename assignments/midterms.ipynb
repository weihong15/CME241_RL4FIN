{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Value Iteration Optimization\n",
    "Given a maze, find value function of all valid states to terminal states"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define state space, action space, state transition probability function, two different reward transition function and discount rate\n",
    "\n",
    "For the following below, we are formulating for a general maze. with maze_grid given as input(defined by dictionary of position to (space, block, goal) )  \n",
    "\n",
    "1. **State Space**: $\\mathcal{S}$ = { (i,j) | maze_grid[(i,j)] == \"SPACE\" or maze_grid[(i,j)] == \"GOAL\"}\n",
    "2. Action Space, We first define the following set. U = {\"up\", \"down\", \"left\", \"right\"} action_map = {\"left\":(0,-1),\"right\":(0,1),\"up\":(-1,0),\"down\":(1,0)}. Note that maze_grid is a given set in grid_maze.py. \n",
    "\n",
    "**Action Space** will be defined as: $\\mathcal{A}(s)$ = {a| $a\\in U$,  maze_grid[s+action_map[a]] == \"SPACE\" or maze_grid[s+action_map[a]] == \"GOAL\"}\n",
    "\n",
    "\n",
    "3. **State Transition Probability Function**: $\\mathcal{P}(s,a,s')$. Given s and s' are valid state (i.e. mazegrid[s] == \"SPACE\" and mazegrid[s'] == \"SPACE\")\n",
    "\\begin{equation}\n",
    "\\mathcal{P}((i,j),\"up\",(i,j-1)) = 1 \\\\\n",
    "\\mathcal{P}((i,j),\"down\",(i,j+1)) = 1\\\\\n",
    "\\mathcal{P}((i,j),\"left\",(i-1,j)) = 1\\\\\n",
    "\\mathcal{P}((i,j),\"right\",(i+1,j)) = 1 \\\\\n",
    "\\text{otherwise, it will be 0}\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "4. **Reward transition function 1**: all rewards are 0 except when reaching the end goal. i.e.\n",
    "\\begin{equation}\n",
    "\\mathcal{R}_T(\\cdot,\\cdot,(7,7)) = 1\n",
    "\\end{equation}\n",
    "This case we will have discount rate = $\\gamma <1$\n",
    "\n",
    "5. **Reward transition function 2**: each move we take, we waste energy and take a cost of -0.01. End goal will still have reward of 1. \n",
    "\\begin{equation}\n",
    "\\mathcal{R}_T(s,\\cdot,s') = -0.01 \\text{ if $s'\\neq (7,7)$}\\\\\n",
    "\\mathcal{R}_T(\\cdot,\\cdot,(7,7)) = 1\n",
    "\\end{equation}\n",
    "This case we can have discount rate = $\\gamma=1$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from grid_maze import *\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "from collections import defaultdict\n",
    "from rl.markov_decision_process import  StateActionMapping, FinitePolicy, FiniteMarkovDecisionProcess\n",
    "from rl.markov_process import FiniteMarkovRewardProcess\n",
    "from rl.markov_process import StateReward\n",
    "from rl.distribution import Choose,Constant,Categorical\n",
    "from dataclasses import dataclass\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from rl.dynamic_programming import policy_iteration_result, value_iteration_result\n",
    "from typing import (Text, Dict, Iterable, Generic, Sequence, Tuple,\n",
    "                    Mapping, Optional, TypeVar, Generator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Formulate the MDP, solve optimal value, optimal policy and find the number of iterations needed to converge\n",
    "\n",
    "edited code in rl/iterate, line 43 - 65\n",
    "```\n",
    "def converge(values: Iterator[X], done: Callable[[X, X], bool]) -> Iterator[X]:\n",
    "    '''Read from an iterator until two consecutive values satisfy the\n",
    "    given done function or the input iterator ends.\n",
    "\n",
    "    Raises an error if the input iterator is empty.\n",
    "\n",
    "    Will loop forever if the input iterator doesn't end *or* converge.\n",
    "\n",
    "    '''\n",
    "    a = next(values, None)\n",
    "    if a is None:\n",
    "        return\n",
    "    i = 1\n",
    "    yield a\n",
    "\n",
    "    for b in values:\n",
    "        if done(a, b):\n",
    "            print(\"iterations made = \",i)\n",
    "            return\n",
    "\n",
    "        a = b\n",
    "        i +=1\n",
    "        yield b\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tuple_sum(a,b):\n",
    "    return (a[0]+b[0],a[1]+b[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getMappingOne(maze_grid) -> Dict[Tuple[int, int], Dict[Text, Categorical[Tuple[int, int]]]]:\n",
    "    N: Sequence[Tuple[int, int]] = []\n",
    "    T: Sequence[Tuple[int, int]] = []\n",
    "    B: Sequence[Tuple[int, int]] = []\n",
    "    for k,v in maze_grid.items():\n",
    "        if v == \"SPACE\":\n",
    "            N.append(k)\n",
    "        elif v == \"BLOCK\":\n",
    "            B.append(k)\n",
    "        else:\n",
    "            T.append(k)\n",
    "    S: Sequence[Tuple[int, int]] = N+T\n",
    "    actions = [\"up\", \"down\", \"left\", \"right\"]\n",
    "    a_d = {\"left\":(0,-1),\"right\":(0,1),\"up\":(-1,0),\"down\":(1,0)}\n",
    "    mapping: Dict[Tuple[int, int], Dict[Text, Categorical[Tuple[int, int]]]] ={}\n",
    "    for _ in T:\n",
    "        mapping[_] = None\n",
    "    \n",
    "    for s0 in N:\n",
    "        mapping[s0]:Dict[Text, Categorical[Tuple[int, int]]]={}\n",
    "        for a in actions:\n",
    "            if tuple_sum(s0,a_d[a]) in S:\n",
    "                if tuple_sum(s0,a_d[a]) in T:\n",
    "                    d: Dict[Tuple[Tuple[int, int]],float] = {(tuple_sum(s0,a_d[a]),1):1}\n",
    "                else:  ## s1 is in N\n",
    "                    d: Dict[Tuple[Tuple[int, int]],float] = {(tuple_sum(s0,a_d[a]),0):1}\n",
    "                mapping[s0][a]= Categorical(d)\n",
    "\n",
    "    return mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iterations made =  17\n"
     ]
    }
   ],
   "source": [
    "mdp = FiniteMarkovDecisionProcess(getMappingOne(maze_grid))\n",
    "value_star1, policy_star1 = value_iteration_result(mdp, gamma=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(0, 0): 0.2058911320946491,\n",
       " (0, 2): 0.31381059609000017,\n",
       " (0, 3): 0.34867844010000015,\n",
       " (0, 4): 0.31381059609000017,\n",
       " (0, 5): 0.28242953648100017,\n",
       " (0, 6): 0.25418658283290013,\n",
       " (0, 7): 0.22876792454961012,\n",
       " (1, 0): 0.22876792454961012,\n",
       " (1, 3): 0.38742048900000015,\n",
       " (2, 0): 0.25418658283290013,\n",
       " (2, 2): 0.38742048900000015,\n",
       " (2, 3): 0.43046721000000016,\n",
       " (2, 4): 0.47829690000000014,\n",
       " (2, 5): 0.5314410000000002,\n",
       " (2, 7): 0.5314410000000002,\n",
       " (3, 0): 0.28242953648100017,\n",
       " (3, 1): 0.31381059609000017,\n",
       " (3, 2): 0.34867844010000015,\n",
       " (3, 5): 0.5904900000000002,\n",
       " (3, 7): 0.5904900000000002,\n",
       " (4, 0): 0.25418658283290013,\n",
       " (4, 2): 0.31381059609000017,\n",
       " (4, 4): 0.5904900000000002,\n",
       " (4, 5): 0.6561000000000001,\n",
       " (4, 6): 0.7290000000000001,\n",
       " (4, 7): 0.6561000000000001,\n",
       " (5, 2): 0.28242953648100017,\n",
       " (5, 4): 0.5314410000000002,\n",
       " (5, 6): 0.81,\n",
       " (6, 0): 0.25418658283290013,\n",
       " (6, 4): 0.47829690000000014,\n",
       " (6, 6): 0.9,\n",
       " (6, 7): 1.0,\n",
       " (7, 0): 0.28242953648100017,\n",
       " (7, 1): 0.31381059609000017,\n",
       " (7, 2): 0.34867844010000015,\n",
       " (7, 3): 0.38742048900000015,\n",
       " (7, 4): 0.43046721000000016}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "value_star1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "For State (0, 0):\n",
       "  Do Action down with Probability 1.000\n",
       "For State (0, 2):\n",
       "  Do Action right with Probability 1.000\n",
       "For State (0, 3):\n",
       "  Do Action down with Probability 1.000\n",
       "For State (0, 4):\n",
       "  Do Action left with Probability 1.000\n",
       "For State (0, 5):\n",
       "  Do Action left with Probability 1.000\n",
       "For State (0, 6):\n",
       "  Do Action left with Probability 1.000\n",
       "For State (0, 7):\n",
       "  Do Action left with Probability 1.000\n",
       "For State (1, 0):\n",
       "  Do Action down with Probability 1.000\n",
       "For State (1, 3):\n",
       "  Do Action down with Probability 1.000\n",
       "For State (2, 0):\n",
       "  Do Action down with Probability 1.000\n",
       "For State (2, 2):\n",
       "  Do Action right with Probability 1.000\n",
       "For State (2, 3):\n",
       "  Do Action right with Probability 1.000\n",
       "For State (2, 4):\n",
       "  Do Action right with Probability 1.000\n",
       "For State (2, 5):\n",
       "  Do Action down with Probability 1.000\n",
       "For State (2, 7):\n",
       "  Do Action down with Probability 1.000\n",
       "For State (3, 0):\n",
       "  Do Action right with Probability 1.000\n",
       "For State (3, 1):\n",
       "  Do Action right with Probability 1.000\n",
       "For State (3, 2):\n",
       "  Do Action up with Probability 1.000\n",
       "For State (3, 5):\n",
       "  Do Action down with Probability 1.000\n",
       "For State (3, 7):\n",
       "  Do Action down with Probability 1.000\n",
       "For State (4, 0):\n",
       "  Do Action up with Probability 1.000\n",
       "For State (4, 2):\n",
       "  Do Action up with Probability 1.000\n",
       "For State (4, 4):\n",
       "  Do Action right with Probability 1.000\n",
       "For State (4, 5):\n",
       "  Do Action right with Probability 1.000\n",
       "For State (4, 6):\n",
       "  Do Action down with Probability 1.000\n",
       "For State (4, 7):\n",
       "  Do Action left with Probability 1.000\n",
       "For State (5, 2):\n",
       "  Do Action up with Probability 1.000\n",
       "For State (5, 4):\n",
       "  Do Action up with Probability 1.000\n",
       "For State (5, 6):\n",
       "  Do Action down with Probability 1.000\n",
       "For State (6, 0):\n",
       "  Do Action down with Probability 1.000\n",
       "For State (6, 4):\n",
       "  Do Action up with Probability 1.000\n",
       "For State (6, 6):\n",
       "  Do Action right with Probability 1.000\n",
       "For State (6, 7):\n",
       "  Do Action down with Probability 1.000\n",
       "For State (7, 0):\n",
       "  Do Action right with Probability 1.000\n",
       "For State (7, 1):\n",
       "  Do Action right with Probability 1.000\n",
       "For State (7, 2):\n",
       "  Do Action right with Probability 1.000\n",
       "For State (7, 3):\n",
       "  Do Action right with Probability 1.000\n",
       "For State (7, 4):\n",
       "  Do Action up with Probability 1.000"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy_star1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getMappingTwo(maze_grid)-> Dict[Tuple[int, int], Dict[Text, Categorical[Tuple[int, int]]]]:\n",
    "    N = []\n",
    "    T = []\n",
    "    B = []\n",
    "    for k,v in maze_grid.items():\n",
    "        if v == \"SPACE\":\n",
    "            N.append(k)\n",
    "        elif v == \"BLOCK\":\n",
    "            B.append(k)\n",
    "        else:\n",
    "            T.append(k)\n",
    "    S = N+T\n",
    "    actions = [\"up\", \"down\", \"left\", \"right\"]\n",
    "    a_d = {\"left\":(0,-1),\"right\":(0,1),\"up\":(-1,0),\"down\":(1,0)}\n",
    "    rewards = {i:-0.01 for i in N}\n",
    "    mapping={}\n",
    "    for _ in T:\n",
    "        rewards[_] = 1\n",
    "        mapping[_] = None\n",
    "    \n",
    "    for s0 in N:\n",
    "        mapping[s0]={}\n",
    "        for a in actions:\n",
    "            if tuple_sum(s0,a_d[a]) in S:\n",
    "                if tuple_sum(s0,a_d[a]) in T:\n",
    "                    d = {(tuple_sum(s0,a_d[a]),1):1}\n",
    "                else:  ## s1 is in N\n",
    "                    d = {(tuple_sum(s0,a_d[a]),-0.01):1}\n",
    "                mapping[s0][a]= Categorical(d)\n",
    "\n",
    "    return mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iterations made =  17\n"
     ]
    }
   ],
   "source": [
    "mdp = FiniteMarkovDecisionProcess(getMappingTwo(maze_grid))\n",
    "value_star2, policy_star2 = value_iteration_result(mdp, gamma=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(0, 0): 0.8499999999999999,\n",
       " (0, 2): 0.8899999999999999,\n",
       " (0, 3): 0.8999999999999999,\n",
       " (0, 4): 0.8899999999999999,\n",
       " (0, 5): 0.8799999999999999,\n",
       " (0, 6): 0.8699999999999999,\n",
       " (0, 7): 0.8599999999999999,\n",
       " (1, 0): 0.8599999999999999,\n",
       " (1, 3): 0.9099999999999999,\n",
       " (2, 0): 0.8699999999999999,\n",
       " (2, 2): 0.9099999999999999,\n",
       " (2, 3): 0.9199999999999999,\n",
       " (2, 4): 0.9299999999999999,\n",
       " (2, 5): 0.94,\n",
       " (2, 7): 0.94,\n",
       " (3, 0): 0.8799999999999999,\n",
       " (3, 1): 0.8899999999999999,\n",
       " (3, 2): 0.8999999999999999,\n",
       " (3, 5): 0.95,\n",
       " (3, 7): 0.95,\n",
       " (4, 0): 0.8699999999999999,\n",
       " (4, 2): 0.8899999999999999,\n",
       " (4, 4): 0.95,\n",
       " (4, 5): 0.96,\n",
       " (4, 6): 0.97,\n",
       " (4, 7): 0.96,\n",
       " (5, 2): 0.8799999999999999,\n",
       " (5, 4): 0.94,\n",
       " (5, 6): 0.98,\n",
       " (6, 0): 0.8699999999999999,\n",
       " (6, 4): 0.9299999999999999,\n",
       " (6, 6): 0.99,\n",
       " (6, 7): 1.0,\n",
       " (7, 0): 0.8799999999999999,\n",
       " (7, 1): 0.8899999999999999,\n",
       " (7, 2): 0.8999999999999999,\n",
       " (7, 3): 0.9099999999999999,\n",
       " (7, 4): 0.9199999999999999}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "value_star2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "For State (0, 0):\n",
       "  Do Action down with Probability 1.000\n",
       "For State (0, 2):\n",
       "  Do Action right with Probability 1.000\n",
       "For State (0, 3):\n",
       "  Do Action down with Probability 1.000\n",
       "For State (0, 4):\n",
       "  Do Action left with Probability 1.000\n",
       "For State (0, 5):\n",
       "  Do Action left with Probability 1.000\n",
       "For State (0, 6):\n",
       "  Do Action left with Probability 1.000\n",
       "For State (0, 7):\n",
       "  Do Action left with Probability 1.000\n",
       "For State (1, 0):\n",
       "  Do Action down with Probability 1.000\n",
       "For State (1, 3):\n",
       "  Do Action down with Probability 1.000\n",
       "For State (2, 0):\n",
       "  Do Action down with Probability 1.000\n",
       "For State (2, 2):\n",
       "  Do Action right with Probability 1.000\n",
       "For State (2, 3):\n",
       "  Do Action right with Probability 1.000\n",
       "For State (2, 4):\n",
       "  Do Action right with Probability 1.000\n",
       "For State (2, 5):\n",
       "  Do Action down with Probability 1.000\n",
       "For State (2, 7):\n",
       "  Do Action down with Probability 1.000\n",
       "For State (3, 0):\n",
       "  Do Action right with Probability 1.000\n",
       "For State (3, 1):\n",
       "  Do Action right with Probability 1.000\n",
       "For State (3, 2):\n",
       "  Do Action up with Probability 1.000\n",
       "For State (3, 5):\n",
       "  Do Action down with Probability 1.000\n",
       "For State (3, 7):\n",
       "  Do Action down with Probability 1.000\n",
       "For State (4, 0):\n",
       "  Do Action up with Probability 1.000\n",
       "For State (4, 2):\n",
       "  Do Action up with Probability 1.000\n",
       "For State (4, 4):\n",
       "  Do Action right with Probability 1.000\n",
       "For State (4, 5):\n",
       "  Do Action right with Probability 1.000\n",
       "For State (4, 6):\n",
       "  Do Action down with Probability 1.000\n",
       "For State (4, 7):\n",
       "  Do Action left with Probability 1.000\n",
       "For State (5, 2):\n",
       "  Do Action up with Probability 1.000\n",
       "For State (5, 4):\n",
       "  Do Action up with Probability 1.000\n",
       "For State (5, 6):\n",
       "  Do Action down with Probability 1.000\n",
       "For State (6, 0):\n",
       "  Do Action down with Probability 1.000\n",
       "For State (6, 4):\n",
       "  Do Action up with Probability 1.000\n",
       "For State (6, 6):\n",
       "  Do Action right with Probability 1.000\n",
       "For State (6, 7):\n",
       "  Do Action down with Probability 1.000\n",
       "For State (7, 0):\n",
       "  Do Action right with Probability 1.000\n",
       "For State (7, 1):\n",
       "  Do Action right with Probability 1.000\n",
       "For State (7, 2):\n",
       "  Do Action right with Probability 1.000\n",
       "For State (7, 3):\n",
       "  Do Action right with Probability 1.000\n",
       "For State (7, 4):\n",
       "  Do Action up with Probability 1.000"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy_star2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demonstrate that both policy matches \n",
    "Note that both require 17 iterations, the answers are printed above.  \n",
    "And both policies matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policyEqual(policy_star1,policy_star2):\n",
    "    if policy_star1.states()!=policy_star2.states():\n",
    "        return False\n",
    "    for s0 in policy_star1.states():\n",
    "        if policy_star1.act(s0)!=policy_star2.act(s0):\n",
    "            return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policyEqual(policy_star1,policy_star2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. MRP Value Function Approximation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The bellman equation in vector form is\n",
    "\\begin{equation}\n",
    "v = \\mathcal{R}+\\gamma\\mathcal{P}\\cdot v\n",
    "\\end{equation}\n",
    "\n",
    "where v and $\\mathcal{R}$ is n X 1 vector, $\\mathcal{P}$ is a n X n matrix,  \n",
    "Manipulating the above equation, we will get\n",
    "\\begin{equation}\n",
    "v=(I_n -\\gamma \\mathcal{P})^{-1}\\cdot \\mathcal{R}\n",
    "\\end{equation}\n",
    "\n",
    "$\\Phi \\cdot \\omega$ should be linear to v and therefore linear to $(I_n -\\gamma \\mathcal{P})^{-1}\\cdot \\mathcal{R}$.  \n",
    "In summary, We need to have the following condition\n",
    "* m = n\n",
    "\n",
    "* $(I_n -\\gamma \\mathcal{P})$ should be invertible\n",
    "\n",
    "* $v = (I_n -\\gamma \\mathcal{P})^{-1}\\cdot \\mathcal{R}$ should be in the column space of $\\Phi$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Career Optimization\n",
    "\n",
    "## specify states, actions, rewards, state-transition probabilities and discount factor\n",
    "* States: we will only store an integer, the last gotten hourly wages. {$w | 0\\leq w\\leq W, w\\in\\mathbb{Z}$}\n",
    "* Actions: (l,s) where l is number of hours spend on learning and s is number of hours spend on searching a job  \n",
    "  Actions: {$(l,s)|0\\leq l,s \\leq H, l+s\\leq H$}\n",
    "  \n",
    "* Rewards: rewards will just be an integer, it will be the expected reward at the end of day, computed by w*(H-l-s).  $\\mathcal{R}_T(w0,(l,s),w1) = w1*(H-l-s)$\n",
    "\n",
    "* State-Transition probabilities, we let $x\\sim Po(\\alpha*l)$, then if $w_1,w_2 \\leq W$\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathcal{P}(w0,(l,s),w1) = \n",
    "\\begin{cases} \n",
    "P(x=w1-w0) & \\text{if $w1-w0>1$} \\\\\n",
    "P(x=0)*\\frac{\\beta \\cdot s}{H}    & \\text{if $w1=w0$}\\\\\n",
    "P(x=1)+P(x=0) *(1-\\frac{\\beta \\cdot s}{H}) & \\text{ if $w1=w0+1$}\n",
    "\\end{cases}\n",
    "\\end{equation}\n",
    "\n",
    "* Gamma: discounting factor, we let it be 0.95"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement MDP in python "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass(frozen=True)\n",
    "class actions:\n",
    "    l: int\n",
    "    s: int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import poisson\n",
    "\n",
    "alpha = 0.08\n",
    "beta = 0.82\n",
    "gamma = 0.95\n",
    "H=10\n",
    "W=30\n",
    "\n",
    "def get_action_transition_reward_map()-> Dict[int, Dict[actions, Categorical[Tuple[int,float]]]]:\n",
    "    d: Dict[int, Dict[actions, Categorical[Tuple[int,float]]]] = {}\n",
    "    for w in range(W+1):\n",
    "        d1: Dict[actions, Categorical[Tuple[int, float]]] = {}\n",
    "        for l in range(H+1):\n",
    "            for s in range(H+1-l):\n",
    "                work_hours= H-l-s\n",
    "                reward = w*(work_hours)\n",
    "                poisson_dist=poisson(alpha*l)\n",
    "                \n",
    "                prob_dict: Dict[Tuple[int, float], float] ={}\n",
    "                new_job_wage = min(w+1,W)\n",
    "                new_job_prob = beta*s/H\n",
    "                for adjustment in range(W-w+1):\n",
    "                    if adjustment==W-w:\n",
    "                        probability = 1-poisson_dist.cdf(W-w-1)\n",
    "                    else:\n",
    "                        probability = poisson_dist.pmf(adjustment)\n",
    "                    old_job_wage = w+adjustment\n",
    "                    if new_job_wage>old_job_wage:\n",
    "                        prob_dict[(new_job_wage,work_hours*new_job_wage)]=new_job_prob*probability\n",
    "                        prob_dict[(old_job_wage,work_hours*old_job_wage)]=(1-new_job_prob)*probability\n",
    "                        #basically if new job better, with probability u accept, 1-p is old pay.\n",
    "                    else:\n",
    "                        prob_dict[(old_job_wage,work_hours*old_job_wage)]=probability\n",
    "                d1[actions(l,s)] = Categorical(prob_dict)\n",
    "        d[w] = d1\n",
    "    return d\n",
    "                ## loop over all poisson from 0 till W-w\n",
    "                ## find new wage from poisson.\n",
    "                ## next day state form with the reward\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solve for optimal value function and optimal policy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iterations made =  337\n"
     ]
    }
   ],
   "source": [
    "careerMDP = FiniteMarkovDecisionProcess(get_action_transition_reward_map())\n",
    "value_star1, policy_star1 = value_iteration_result(careerMDP, gamma=gamma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot a graph of the optimal policy or print it. provide an intitive explanation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For State 0:\n",
      "  Do Action actions(l=10, s=0) with Probability 1.000\n",
      "For State 1:\n",
      "  Do Action actions(l=10, s=0) with Probability 1.000\n",
      "For State 2:\n",
      "  Do Action actions(l=10, s=0) with Probability 1.000\n",
      "For State 3:\n",
      "  Do Action actions(l=10, s=0) with Probability 1.000\n",
      "For State 4:\n",
      "  Do Action actions(l=10, s=0) with Probability 1.000\n",
      "For State 5:\n",
      "  Do Action actions(l=10, s=0) with Probability 1.000\n",
      "For State 6:\n",
      "  Do Action actions(l=10, s=0) with Probability 1.000\n",
      "For State 7:\n",
      "  Do Action actions(l=10, s=0) with Probability 1.000\n",
      "For State 8:\n",
      "  Do Action actions(l=10, s=0) with Probability 1.000\n",
      "For State 9:\n",
      "  Do Action actions(l=10, s=0) with Probability 1.000\n",
      "For State 10:\n",
      "  Do Action actions(l=10, s=0) with Probability 1.000\n",
      "For State 11:\n",
      "  Do Action actions(l=10, s=0) with Probability 1.000\n",
      "For State 12:\n",
      "  Do Action actions(l=10, s=0) with Probability 1.000\n",
      "For State 13:\n",
      "  Do Action actions(l=8, s=0) with Probability 1.000\n",
      "For State 14:\n",
      "  Do Action actions(l=5, s=0) with Probability 1.000\n",
      "For State 15:\n",
      "  Do Action actions(l=3, s=0) with Probability 1.000\n",
      "For State 16:\n",
      "  Do Action actions(l=0, s=0) with Probability 1.000\n",
      "For State 17:\n",
      "  Do Action actions(l=0, s=0) with Probability 1.000\n",
      "For State 18:\n",
      "  Do Action actions(l=0, s=0) with Probability 1.000\n",
      "For State 19:\n",
      "  Do Action actions(l=0, s=0) with Probability 1.000\n",
      "For State 20:\n",
      "  Do Action actions(l=0, s=0) with Probability 1.000\n",
      "For State 21:\n",
      "  Do Action actions(l=0, s=0) with Probability 1.000\n",
      "For State 22:\n",
      "  Do Action actions(l=0, s=0) with Probability 1.000\n",
      "For State 23:\n",
      "  Do Action actions(l=0, s=0) with Probability 1.000\n",
      "For State 24:\n",
      "  Do Action actions(l=0, s=0) with Probability 1.000\n",
      "For State 25:\n",
      "  Do Action actions(l=0, s=0) with Probability 1.000\n",
      "For State 26:\n",
      "  Do Action actions(l=0, s=0) with Probability 1.000\n",
      "For State 27:\n",
      "  Do Action actions(l=0, s=0) with Probability 1.000\n",
      "For State 28:\n",
      "  Do Action actions(l=0, s=0) with Probability 1.000\n",
      "For State 29:\n",
      "  Do Action actions(l=0, s=0) with Probability 1.000\n",
      "For State 30:\n",
      "  Do Action actions(l=0, s=0) with Probability 1.000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(policy_star1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, if your salary is low, we should boost our salary by learning more. when your salary becomes higher, it is more worth it to work more than to learn. It is not worth it to search for new job as the pay rise of the new job has a maximum of 1 dollar/hour and there's an uncertainty on whether we are going to get a new job offer. Besides, it's better to spend it on learning more things so we can get a pay raise from the old job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
